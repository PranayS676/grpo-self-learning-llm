# grpo-self-learning-llm

# ðŸš€ Train Your Own R1 Reasoning Model with Unsloth (GRPO)

![Unsloth GRPO](https://your-image-link-here.com) <!-- Add a relevant image or logo -->

## ðŸ§  Introduction  
Unsloth introduces **Group Relative Policy Optimization (GRPO)**, enabling **efficient reasoning model training** with up to **80% less VRAM** than traditional methods. Now, you can **train a reasoning model with just 7GB VRAM**, reproducing the "aha moment" of DeepSeek R1-Zero.

## ðŸŒŸ Features  
âœ” **Optimized GRPO** â€“ Efficient reinforcement learning for reasoning models  
âœ” **Low VRAM Requirement** â€“ Train models with as little as **7GB VRAM**  
âœ” **Broad Model Support** â€“ Fine-tune **Llama 3.1 (8B), Phi-4 (14B), Mistral (7B), Qwen2.5 (7B)**  
âœ” **LoRA & QLoRA Compatible** â€“ Works with efficient fine-tuning techniques  
âœ” **Self-Learning Reasoning** â€“ Models **autonomously allocate more thinking time**  

## ðŸ“Œ Quick Start  

### ðŸ”¹ 1. Install Dependencies  
```bash
pip install unsloth vllm diffusers
